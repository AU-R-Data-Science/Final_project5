---
title: "Final_Project"
author: "Shawn Yates"
date: "2022-11-14"
output: html_document
---

```{r}
library(ggplot2)
library(dplyr)
library(caret)
library(lattice)
```

```{r}
setwd("~/GitHub/Final_project5")
data=read.csv("crop.data.csv")


#' Pi function
#' Defined as p, inverse of logit
#' @param z numerical dataset to be fitted to a logistic regression curve 
#'
#' @return logistic regression curve 
#' @export
#'
#' @examples Pi(data)
Pi <- function(z){
  Pi_result<-1/(1+exp(-z))
  return(Pi_result)
  } 



#' Objective function to be minimized
#'
#' @param beta the coefficient vector 
#' @param X a vector of numerical 
#' @param y 
#'
#' @return returns the loss function??
#' @export
#'
#' @examples
obj_fn <- function(beta, X, y){
  n <- length(y) # number of training examples
  p <- Pi(X%*%beta)
  J <- (t(-y)%*%log(p)-t(1-y)%*%log(1-p))/n
  return(J)
}

```

```{r}

#' Gradient function
#'
#' @param beta 
#' @param X 
#' @param y 
#'
#' @return
#' @export
#'
#' @examples
grad <- function(beta, X, y){
  n <- length(y)
  p <- Pi(X%*%beta)
  grad <- (t(X)%*%(p - y))/n
  grad
}
```

```{r}
#' Title
#'
#' @param X 
#' @param y 
#'
#' @return
#' @export
#'
#' @examples
beta_hat <- function(X, y){
  X <- as.matrix(X)
  n <- nrow(X)
  intercept <- rep(1,n)
  X <- cbind(intercept,X)
  #initialize beta
  beta <- solve(t(X)%*%X)%*%(t(X)%*%y)
  #use the optim function to perform gradient descent
  obj_fnOpti <- optim(beta, obj_fn, grad, X=X, y=y)
  #return coefficients
  return(obj_fnOpti$par)
}
```
##### Training our model with our generated dataset
```{r}
set.seed(8)
y <- sample(c(0,1), size = 20, replace = TRUE)
n <- length(y)
x1 <- rpois(n, lambda = 3)
x2 <- rnorm(n, -3, 5)
x3 <- rexp(n, rate = 2)
# int <- rep(1, n)
X <- cbind(x1, x2, x3)
data <- data.frame(y, X)
data.X <- data[, -1]
data.y <- data[, 1]
```

```{r}
model <- beta_hat(data.X, data.y)
model
```
The first number is the intercept. The next three numbers are the coefficient of for `x1, x2`  and  `x3`. These describe their log odds.


##### Comparing our self-built model with `glm()` function.
```{r}
true.model <- glm(y ~ x1 + x2 + x3, family=binomial(link ="logit"), data = data)
summary(true.model)$coefficients
```

##### Prediction function.
```{r}
#' Title
#'
#' @param model 
#' @param X 
#'
#' @return
#' @export
#'
#' @examples
bhat.predict <- function(model, X){
  X <- as.matrix(X)
  n <- nrow(X)
  intercept <- rep(1, n)
  X <- cbind(intercept,X)
  return(Pi(X%*%model))
}
```

##### Function for the confusion matrix
```{r}
#' Title
#'
#' @param X 
#' @param y 
#'
#' @return
#' @export
#'
#' @examples
confM <- function(X, y){
  y.prediction <- bhat.predict(model, X)
  glm.pred <- rep ("0",length(y))
  glm.pred[y.prediction > 0.5]="1"
  gpp <- factor(glm.pred)
  newy <- factor(y)
 return(confusionMatrix(data = gpp, reference = newy)) 
}
```

#####  Test data.
```{r}
set.seed(51)
n = length(y)
newdata <- data.frame(x1 =rpois(n,lambda = 3), x2 =runif(n,-1, 1), x3 = rexp(n, rate= 5) )
newy <- sample(c(0,1), size = n, replace = TRUE)
```

##### Outputs from the confusion matrix
```{r}
conf.mat <- confM(newdata,newy)
paste("False Discovery Rate is", conf.mat$table[1,2]/(conf.mat$table[1,1] + conf.mat$table[1,2]))
paste("Diagnositic odds Ratio is", (conf.mat$table[1,1]*conf.mat$table[2,2])/(conf.mat$table[1,2]*conf.mat$table[2,1]))
conf.mat


```
#####  boostrap confidence interval 
```{r}
Boostrap <- function(X, y, alpha = 0.05, R = 20, seed = NULL, ...){
    #Calculate the mean
    X <- as.matrix(X)
    n <- nrow(X)
    p <- ncol(X) + 1
    coeff.boot <- matrix(NA, nrow = R, ncol = p)
    for (k in 1:R){
      row <- sample(1:n, replace = TRUE)
      X.sample <- X[row, ]
      y.sample <- y[row]
      coeff.boot[k,] <- beta_hat(X.sample, y.sample)}
    beta.mean <- apply(coeff.boot, 2, mean)
  beta.sd <- apply(coeff.boot, 2, sd)
  Lower.conf <- beta.mean - qnorm(1-alpha/2)*beta.sd
  Upper.conf <- beta.mean + qnorm(1-alpha/2)*beta.sd
  conf.interval <- cbind(beta.mean, beta.sd, Lower.conf, Upper.conf)
  return(conf.interval)
}


```




