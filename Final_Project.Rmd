---
title: "Final_Project"
author: "Shawn Yates"
date: "2022-11-14"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
#setwd("~/GitHub/Final_project5")
data <- read.csv("~/Documents/GitHub/Final_project5/crop.data.csv")
summary(data)
nrow(data)
library(ggplot2)
library(dplyr)
```

```{r}
#sigmoid function, inverse of logit
sigmoid <- function(z){1/(1+exp(-z))}
#cost function
cost <- function(theta, X, y){
  m <- length(y) # number of training examples
  h <- sigmoid(X %*% theta)
  J <- (t(-y)%*%log(h)-t(1-y)%*%log(1-h))/m
  J
}
# gradient vector
grad <- function(theta, X, y){
  m <- length(y) 
  
  h <- sigmoid(X%*%theta)
  grad <- (t(X)%*%(h - y))/m
  grad
}
```
```{r, message=FALSE, warning=FALSE}
logisticReg <- function(X, y){
  #remove NA rows
  X <- na.omit(X)
  y <- na.omit(y)
  #add bias term and convert to matrix
  X <- mutate(X, bias = 1)
  #move the bias column to col1
  X <- as.matrix(X[, c(ncol(X), 1:(ncol(X)-1))])
  y <- as.matrix(y)
  #initialize theta
  theta <- matrix(rep(0, ncol(X)), nrow = ncol(X))
  #use the optim function to perform gradient descent
  costOpti <- optim(theta, fn = cost, gr = grad, X = X, y = y)
  #return coefficients
  return(costOpti$par)
}
```

```{r, message=FALSE, warning=FALSE}
# probability of getting 1
logisticProb <- function(theta, X){
  X <- na.omit(X)
  #add bias term and convert to matrix
  X <- mutate(X, bias =1)
  X <- as.matrix(X[,c(ncol(X), 1:(ncol(X)-1))])
  return(sigmoid(X%*%theta))
}
# y prediction
logisticPred <- function(prob){
  return(round(prob, 0))
}
```


```{r, message=FALSE, warning=FALSE}
N <- 96 # number of points per class
D <- 2 # dimensionality, we use 2D data for easy visualization
K <- 2 # number of classes, binary for logistic regression
X <- data.frame() # data matrix (each row = single example, can view as xy coordinates)
y <- data.frame() # class labels
set.seed(56)
for (j in (1:K)){
  # t, m are parameters of parametric equations x1, x2
  t <- seq(0,1,length.out = N) 
  # add randomness 
  m <- rnorm(N, j+0.5, 0.25) 
  Xtemp <- data.frame(x1 = 3*t , x2 = m - t) 
  ytemp <- data.frame(matrix(j-1, N, 1))
  X <- rbind(X, Xtemp)
  y <- rbind(y, ytemp)
}
# combine the data
data <- cbind(X,y)
colnames(data) <- c(colnames(X), 'label')
# visualize the data:
ggplot(data) + geom_point(aes(x=x1, y=x2, color = as.character(label)), size = 2) + 
  scale_colour_discrete(name  ="Label") + 
  ylim(0, 3) + coord_fixed(ratio = 1) +
  ggtitle('Data to be classified') +
  theme_bw(base_size = 12) +
  theme(legend.position=c(0.85, 0.87))
```


```{r, message=FALSE, warning=FALSE}
# training
theta <- logisticReg(X, y)
# generate a grid for decision boundary, this is the test set
grid <- expand.grid(seq(0, 3, length.out = 100), seq(0, 3, length.out = 100))
# predict the probability
probZ <- logisticProb(theta, grid)
# predict the label
Z <- logisticPred(probZ)
gridPred = cbind(grid, Z)
```


```{r, message=FALSE, warning=FALSE}
# decision boundary visualization
ggplot() +  geom_point(data = data, aes(x=x1, y=x2, color = as.character(label)), size = 2, show.legend = F) + geom_tile(data = gridPred, aes(x = grid[, 1],y = grid[, 2], fill=as.character(Z)), alpha = 0.3, show.legend = F) + ylim(0, 3) + ggtitle('Decision Boundary for Logistic Regression') + coord_fixed(ratio = 1) + theme_bw(base_size = 12) 
```

from here down this uses an already made package and is not running correctly yet
```{r}
library(ggplot2)

x = c(1:10)
y = as.factor(c(rep(0,5), rep(1,5)))
data = data.frame(x,y)

sigmoid = function(x){
  1/(1+exp(-x))
}

data$function1 = sigmoid(data$x - 5) + 1
data$function2 = sigmoid(data$x -7) +1 

ggplot(data,aes(x)) + 
  geom_point(aes(y = y,col = y),size = 5) +
  geom_line(aes(y = function1), col = "green", linetype = "dashed") +
  geom_line(aes(y = function2), col = "blue", linetype = "dashed") +
  theme_minimal() + theme(legend.position = "bottom")


```


```{r}

sigmoid = function(x){
  1/(1+exp(-x))
}

neg_log_likelihood = function(par, data, y, include_alpha = T){
  x = data[,names(data) != y]
  y_data = data[,y]

  # 1. Calculate theta
  if(include_alpha){

    # Multiply each value by their parameter
    multiplied =  mapply("*",x,par[2:length(par)])

    # We sum for each observation and add alpha
    theta =  rowSums(multiplied) + par[1]
  }else{
    theta =  rowSums(mapply("*",x,par))
  }

  # 2. Calculate p
  p = sigmoid(theta)
  #p = exp(theta) / (1 + exp(theta))

  # 3. Calculate -log likelihood
  val = -sum(y_data * log(p) + (1-y_data)*log(1-p)) 

  return(val)
}

```

```{r}
opt = optim(
  par = c(0,0,0),
  fn = neg_log_likelihood,
  data = data,
  y = "obese",
  include_alpha = T,
  control = list(trace = 0, all.methods = TRUE)
)

opt[,1:5]

```



```{r}
install.packages("caret")
library(caret)

data$coefficients

confusionMatrix(as.factor(round(logisticPred())), as.factor(data$yield))
```
 #### trying to build a confusion matrix, work in progress
 
```{r}
actual_values = matrix(c("T", "T", "T", "T", "F", "F","F"), ncol=1)
predict_value = matrix(c("T", "F", "T", "T", "F", "T","F"), ncol=1)
table(actual_values,predict_value)
```
 



